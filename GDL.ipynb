{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genome Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior() \n",
    "import BRCA_inference\n",
    "import BRCA_batch\n",
    "import BRCA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置神经网络参数\n",
    "# Set Neuronetwork Parameters\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 600\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取路径\n",
    "#Data_Read_PATH = \"/home/sunysh/12Cancer/BRCA/10000-1-vcf/part1/train.Matrix\"\n",
    "\n",
    "Data_Read_PATH = \"C:\\\\Users\\\\Eric\\\\Desktop\\\\Final_project_code_data\\\\10000-1-vcf\\\\10000-1-vcf\\\\part1\\\\train.Matrix\"\n",
    "\n",
    "# 模型保存的路径和文件名\n",
    "#MODEL_SAVE_PATH = \"/home/sunysh/12Cancer/BRCA/10000-1-vcf/part1/Model\"\n",
    "\n",
    "MODEL_SAVE_PATH = \"C:\\\\Users\\\\Eric\\\\Desktop\\\\Final_project_code_data\\\\10000-1-vcf\\\\10000-1-vcf\\\\part1\\\\Model\"\n",
    "MODEL_NAME = \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    # 训练数据读取\n",
    "    L = BRCA_data.readCase(Data_Read_PATH)\n",
    "\n",
    "    #定义输入层\n",
    "    x = tf.placeholder(tf.float32, [None, BRCA_inference.INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, BRCA_inference.OUTPUT_NODE], name='y_input')\n",
    "\n",
    "    # 返回regularizer函数，L2正则化项的值\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    # 使用BRCA_inference.py中定义的前向传播过程\n",
    "    y = BRCA_inference.inference(x, regularizer)\n",
    "    # 定义step为0\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 滑动平均,由衰减率和步数确定\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    # 可训练参数的集合\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # 交叉熵损失函数\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    # 交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    # 总损失\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "    # 学习率(衰减)\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, TRAINING_STEPS, LEARNING_RATE_DECAY)\n",
    "    # 定义了反向传播的优化方法，之后通过sess.run(train_step)就可以对所有GraphKeys.TRAINABLE_VARIABLES集合中的变量进行优化，似的当前batch下损失函数更小\n",
    "    # 实现梯度下降算法的优化器\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    # 更新参数\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #TensorBoard\n",
    "    writer = tf.summary.FileWriter(\"C:\\\\Users\\\\Eric\\\\Desktop\\\\Final_project_code_data\\\\10000-1-vcf\\\\10000-1-vcf\\\\part1\\\\Log\", tf.get_default_graph())\n",
    "    writer.close()\n",
    "\n",
    "    # 初始会话，并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            L = BRCA_data.readCase(Data_Read_PATH)\n",
    "            xs, ys = BRCA_batch.all(L)\n",
    "            op, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            print(global_step.eval(), 'loss:', loss_value)\n",
    "            \n",
    "        saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "\n",
    "        #saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Eric\\Desktop\\Final_project_code_data\\10000-1-vcf\\10000-1-vcf\\part1\\code\\BRCA_inference.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Eric\\Desktop\\Final_project_code_data\\10000-1-vcf\\10000-1-vcf\\part1\\code\\BRCA_inference.py:22: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Eric\\Desktop\\Final_project_code_data\\10000-1-vcf\\10000-1-vcf\\part1\\code\\BRCA_inference.py:26: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Eric\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\training\\moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "1 loss: 1.6909235\n",
      "2 loss: 1.7451932\n",
      "3 loss: 1.5935268\n",
      "4 loss: 1.4768261\n",
      "5 loss: 1.4373531\n",
      "6 loss: 1.4020199\n",
      "7 loss: 1.3684076\n",
      "8 loss: 1.336546\n",
      "9 loss: 1.3070184\n",
      "10 loss: 1.2800237\n",
      "11 loss: 1.255716\n",
      "12 loss: 1.2341719\n",
      "13 loss: 1.2155734\n",
      "14 loss: 1.1997017\n",
      "15 loss: 1.1872715\n",
      "16 loss: 1.1791923\n",
      "17 loss: 1.182256\n",
      "18 loss: 1.2062185\n",
      "19 loss: 1.3378562\n",
      "20 loss: 1.6758127\n",
      "21 loss: 2.3979387\n",
      "22 loss: 1.293473\n",
      "23 loss: 1.2553895\n",
      "24 loss: 1.2307898\n",
      "25 loss: 1.2119154\n",
      "26 loss: 1.1965839\n",
      "27 loss: 1.1838648\n",
      "28 loss: 1.173006\n",
      "29 loss: 1.1636152\n",
      "30 loss: 1.1553606\n",
      "31 loss: 1.1479725\n",
      "32 loss: 1.1412958\n",
      "33 loss: 1.135229\n",
      "34 loss: 1.1296858\n",
      "35 loss: 1.1245753\n",
      "36 loss: 1.1198268\n",
      "37 loss: 1.1154174\n",
      "38 loss: 1.1113063\n",
      "39 loss: 1.1074421\n",
      "40 loss: 1.1038128\n",
      "41 loss: 1.1003972\n",
      "42 loss: 1.097177\n",
      "43 loss: 1.0941315\n",
      "44 loss: 1.0912498\n",
      "45 loss: 1.0885187\n",
      "46 loss: 1.0859332\n",
      "47 loss: 1.083461\n",
      "48 loss: 1.0811019\n",
      "49 loss: 1.0788568\n",
      "50 loss: 1.0767218\n",
      "51 loss: 1.0746676\n",
      "52 loss: 1.0727015\n",
      "53 loss: 1.0708164\n",
      "54 loss: 1.0690122\n",
      "55 loss: 1.0672855\n",
      "56 loss: 1.0656275\n",
      "57 loss: 1.0640366\n",
      "58 loss: 1.0625141\n",
      "59 loss: 1.0610474\n",
      "60 loss: 1.0596446\n",
      "61 loss: 1.058301\n",
      "62 loss: 1.0570173\n",
      "63 loss: 1.0557925\n",
      "64 loss: 1.0546212\n",
      "65 loss: 1.0535014\n",
      "66 loss: 1.0524191\n",
      "67 loss: 1.051374\n",
      "68 loss: 1.0503757\n",
      "69 loss: 1.0494184\n",
      "70 loss: 1.0485039\n",
      "71 loss: 1.0476236\n",
      "72 loss: 1.0467793\n",
      "73 loss: 1.0459692\n",
      "74 loss: 1.0451945\n",
      "75 loss: 1.0444484\n",
      "76 loss: 1.0437311\n",
      "77 loss: 1.0430436\n",
      "78 loss: 1.0423837\n",
      "79 loss: 1.0417491\n",
      "80 loss: 1.0411392\n",
      "81 loss: 1.0405533\n",
      "82 loss: 1.0399891\n",
      "83 loss: 1.0394444\n",
      "84 loss: 1.0389198\n",
      "85 loss: 1.0384135\n",
      "86 loss: 1.0379258\n",
      "87 loss: 1.0374557\n",
      "88 loss: 1.0370018\n",
      "89 loss: 1.036562\n",
      "90 loss: 1.0361363\n",
      "91 loss: 1.035724\n",
      "92 loss: 1.035323\n",
      "93 loss: 1.034934\n",
      "94 loss: 1.0345551\n",
      "95 loss: 1.0341861\n",
      "96 loss: 1.0338272\n",
      "97 loss: 1.0334768\n",
      "98 loss: 1.0331353\n",
      "99 loss: 1.0328015\n",
      "100 loss: 1.0324749\n",
      "101 loss: 1.0321554\n",
      "102 loss: 1.0318428\n",
      "103 loss: 1.0315361\n",
      "104 loss: 1.0312355\n",
      "105 loss: 1.0309402\n",
      "106 loss: 1.0306503\n",
      "107 loss: 1.0303652\n",
      "108 loss: 1.0300852\n",
      "109 loss: 1.0298096\n",
      "110 loss: 1.0295383\n",
      "111 loss: 1.0292711\n",
      "112 loss: 1.0290079\n",
      "113 loss: 1.0287482\n",
      "114 loss: 1.028492\n",
      "115 loss: 1.0282393\n",
      "116 loss: 1.0279895\n",
      "117 loss: 1.0277429\n",
      "118 loss: 1.0274992\n",
      "119 loss: 1.027258\n",
      "120 loss: 1.0270195\n",
      "121 loss: 1.0267837\n",
      "122 loss: 1.0265502\n",
      "123 loss: 1.0263193\n",
      "124 loss: 1.02609\n",
      "125 loss: 1.0258633\n",
      "126 loss: 1.0256387\n",
      "127 loss: 1.0254158\n",
      "128 loss: 1.0251949\n",
      "129 loss: 1.0249755\n",
      "130 loss: 1.0247582\n",
      "131 loss: 1.0245423\n",
      "132 loss: 1.0243281\n",
      "133 loss: 1.0241154\n",
      "134 loss: 1.0239042\n",
      "135 loss: 1.0236943\n",
      "136 loss: 1.0234858\n",
      "137 loss: 1.0232787\n",
      "138 loss: 1.0230728\n",
      "139 loss: 1.0228684\n",
      "140 loss: 1.0226647\n",
      "141 loss: 1.0224624\n",
      "142 loss: 1.0222614\n",
      "143 loss: 1.022061\n",
      "144 loss: 1.0218618\n",
      "145 loss: 1.0216633\n",
      "146 loss: 1.0214661\n",
      "147 loss: 1.0212697\n",
      "148 loss: 1.0210742\n",
      "149 loss: 1.0208795\n",
      "150 loss: 1.0206857\n",
      "151 loss: 1.0204928\n",
      "152 loss: 1.0203005\n",
      "153 loss: 1.0201089\n",
      "154 loss: 1.0199182\n",
      "155 loss: 1.0197281\n",
      "156 loss: 1.0195386\n",
      "157 loss: 1.0193499\n",
      "158 loss: 1.0191618\n",
      "159 loss: 1.0189744\n",
      "160 loss: 1.0187873\n",
      "161 loss: 1.0186012\n",
      "162 loss: 1.0184155\n",
      "163 loss: 1.0182302\n",
      "164 loss: 1.0180455\n",
      "165 loss: 1.0178614\n",
      "166 loss: 1.0176777\n",
      "167 loss: 1.0174946\n",
      "168 loss: 1.0173119\n",
      "169 loss: 1.0171299\n",
      "170 loss: 1.016948\n",
      "171 loss: 1.0167668\n",
      "172 loss: 1.0165862\n",
      "173 loss: 1.0164058\n",
      "174 loss: 1.0162257\n",
      "175 loss: 1.0160459\n",
      "176 loss: 1.0158669\n",
      "177 loss: 1.0156878\n",
      "178 loss: 1.0155095\n",
      "179 loss: 1.0153313\n",
      "180 loss: 1.0151536\n",
      "181 loss: 1.0149761\n",
      "182 loss: 1.014799\n",
      "183 loss: 1.0146222\n",
      "184 loss: 1.0144459\n",
      "185 loss: 1.0142696\n",
      "186 loss: 1.0140939\n",
      "187 loss: 1.0139183\n",
      "188 loss: 1.013743\n",
      "189 loss: 1.013568\n",
      "190 loss: 1.0133934\n",
      "191 loss: 1.0132188\n",
      "192 loss: 1.0130447\n",
      "193 loss: 1.0128709\n",
      "194 loss: 1.0126972\n",
      "195 loss: 1.0125238\n",
      "196 loss: 1.0123507\n",
      "197 loss: 1.0121777\n",
      "198 loss: 1.012005\n",
      "199 loss: 1.0118327\n",
      "200 loss: 1.0116606\n",
      "201 loss: 1.0114884\n",
      "202 loss: 1.0113167\n",
      "203 loss: 1.0111449\n",
      "204 loss: 1.0109738\n",
      "205 loss: 1.0108025\n",
      "206 loss: 1.0106317\n",
      "207 loss: 1.0104607\n",
      "208 loss: 1.0102903\n",
      "209 loss: 1.0101198\n",
      "210 loss: 1.0099494\n",
      "211 loss: 1.0097798\n",
      "212 loss: 1.0096098\n",
      "213 loss: 1.0094402\n",
      "214 loss: 1.0092705\n",
      "215 loss: 1.0091013\n",
      "216 loss: 1.0089324\n",
      "217 loss: 1.0087633\n",
      "218 loss: 1.0085944\n",
      "219 loss: 1.0084258\n",
      "220 loss: 1.0082574\n",
      "221 loss: 1.008089\n",
      "222 loss: 1.0079209\n",
      "223 loss: 1.0077528\n",
      "224 loss: 1.0075846\n",
      "225 loss: 1.0074171\n",
      "226 loss: 1.0072492\n",
      "227 loss: 1.0070816\n",
      "228 loss: 1.0069144\n",
      "229 loss: 1.0067472\n",
      "230 loss: 1.0065801\n",
      "231 loss: 1.006413\n",
      "232 loss: 1.0062462\n",
      "233 loss: 1.0060794\n",
      "234 loss: 1.0059129\n",
      "235 loss: 1.0057465\n",
      "236 loss: 1.0055802\n",
      "237 loss: 1.0054137\n",
      "238 loss: 1.0052476\n",
      "239 loss: 1.0050818\n",
      "240 loss: 1.004916\n",
      "241 loss: 1.0047499\n",
      "242 loss: 1.0045842\n",
      "243 loss: 1.0044187\n",
      "244 loss: 1.0042531\n",
      "245 loss: 1.004088\n",
      "246 loss: 1.0039227\n",
      "247 loss: 1.0037575\n",
      "248 loss: 1.0035925\n",
      "249 loss: 1.0034275\n",
      "250 loss: 1.0032625\n",
      "251 loss: 1.0030978\n",
      "252 loss: 1.0029333\n",
      "253 loss: 1.0027688\n",
      "254 loss: 1.0026042\n",
      "255 loss: 1.0024399\n",
      "256 loss: 1.0022757\n",
      "257 loss: 1.0021116\n",
      "258 loss: 1.0019475\n",
      "259 loss: 1.0017834\n",
      "260 loss: 1.0016195\n",
      "261 loss: 1.0014558\n",
      "262 loss: 1.001292\n",
      "263 loss: 1.0011283\n",
      "264 loss: 1.0009648\n",
      "265 loss: 1.0008012\n",
      "266 loss: 1.0006378\n",
      "267 loss: 1.0004745\n",
      "268 loss: 1.0003114\n",
      "269 loss: 1.000148\n",
      "270 loss: 0.9999851\n",
      "271 loss: 0.999822\n",
      "272 loss: 0.99965894\n",
      "273 loss: 0.9994963\n",
      "274 loss: 0.99933344\n",
      "275 loss: 0.9991707\n",
      "276 loss: 0.9990081\n",
      "277 loss: 0.9988455\n",
      "278 loss: 0.99868304\n",
      "279 loss: 0.9985206\n",
      "280 loss: 0.9983583\n",
      "281 loss: 0.9981961\n",
      "282 loss: 0.99803394\n",
      "283 loss: 0.9978718\n",
      "284 loss: 0.99770975\n",
      "285 loss: 0.99754775\n",
      "286 loss: 0.99738586\n",
      "287 loss: 0.99722403\n",
      "288 loss: 0.99706215\n",
      "289 loss: 0.9969004\n",
      "290 loss: 0.99673873\n",
      "291 loss: 0.99657696\n",
      "292 loss: 0.9964155\n",
      "293 loss: 0.996254\n",
      "294 loss: 0.9960927\n",
      "295 loss: 0.9959313\n",
      "296 loss: 0.99577004\n",
      "297 loss: 0.9956088\n",
      "298 loss: 0.99544764\n",
      "299 loss: 0.9952865\n",
      "300 loss: 0.9951254\n",
      "301 loss: 0.99496436\n",
      "302 loss: 0.9948036\n",
      "303 loss: 0.9946428\n",
      "304 loss: 0.99448186\n",
      "305 loss: 0.9943212\n",
      "306 loss: 0.99416053\n",
      "307 loss: 0.99399984\n",
      "308 loss: 0.99383926\n",
      "309 loss: 0.9936787\n",
      "310 loss: 0.99351823\n",
      "311 loss: 0.99335784\n",
      "312 loss: 0.9931975\n",
      "313 loss: 0.99303705\n",
      "314 loss: 0.99287695\n",
      "315 loss: 0.99271667\n",
      "316 loss: 0.9925565\n",
      "317 loss: 0.9923966\n",
      "318 loss: 0.9922365\n",
      "319 loss: 0.99207664\n",
      "320 loss: 0.9919165\n",
      "321 loss: 0.9917566\n",
      "322 loss: 0.99159694\n",
      "323 loss: 0.9914371\n",
      "324 loss: 0.9912773\n",
      "325 loss: 0.9911177\n",
      "326 loss: 0.990958\n",
      "327 loss: 0.9907985\n",
      "328 loss: 0.990639\n",
      "329 loss: 0.99047965\n",
      "330 loss: 0.99032\n",
      "331 loss: 0.99016076\n",
      "332 loss: 0.99000144\n",
      "333 loss: 0.98984206\n",
      "334 loss: 0.98968273\n",
      "335 loss: 0.98952365\n",
      "336 loss: 0.9893645\n",
      "337 loss: 0.9892055\n",
      "338 loss: 0.98904634\n",
      "339 loss: 0.9888873\n",
      "340 loss: 0.98872846\n",
      "341 loss: 0.98856926\n",
      "342 loss: 0.9884106\n",
      "343 loss: 0.98825157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 loss: 0.98809284\n",
      "345 loss: 0.9879342\n",
      "346 loss: 0.9877755\n",
      "347 loss: 0.98761696\n",
      "348 loss: 0.98745835\n",
      "349 loss: 0.98729974\n",
      "350 loss: 0.98714125\n",
      "351 loss: 0.98698276\n",
      "352 loss: 0.9868244\n",
      "353 loss: 0.98666596\n",
      "354 loss: 0.98650765\n",
      "355 loss: 0.98634946\n",
      "356 loss: 0.9861912\n",
      "357 loss: 0.98603296\n",
      "358 loss: 0.9858747\n",
      "359 loss: 0.98571664\n",
      "360 loss: 0.9855586\n",
      "361 loss: 0.98540056\n",
      "362 loss: 0.9852426\n",
      "363 loss: 0.9850847\n",
      "364 loss: 0.98492676\n",
      "365 loss: 0.98476887\n",
      "366 loss: 0.98461103\n",
      "367 loss: 0.98445326\n",
      "368 loss: 0.9842955\n",
      "369 loss: 0.9841377\n",
      "370 loss: 0.9839802\n",
      "371 loss: 0.9838226\n",
      "372 loss: 0.98366493\n",
      "373 loss: 0.98350734\n",
      "374 loss: 0.98334986\n",
      "375 loss: 0.98319244\n",
      "376 loss: 0.98303497\n",
      "377 loss: 0.98287755\n",
      "378 loss: 0.98272014\n",
      "379 loss: 0.98256296\n",
      "380 loss: 0.9824056\n",
      "381 loss: 0.9822484\n",
      "382 loss: 0.9820911\n",
      "383 loss: 0.98193413\n",
      "384 loss: 0.98177695\n",
      "385 loss: 0.98161983\n",
      "386 loss: 0.9814629\n",
      "387 loss: 0.98130584\n",
      "388 loss: 0.98114884\n",
      "389 loss: 0.98099196\n",
      "390 loss: 0.98083514\n",
      "391 loss: 0.98067826\n",
      "392 loss: 0.9805213\n",
      "393 loss: 0.9803645\n",
      "394 loss: 0.98020786\n",
      "395 loss: 0.9800513\n",
      "396 loss: 0.9798945\n",
      "397 loss: 0.979738\n",
      "398 loss: 0.97958136\n",
      "399 loss: 0.9794247\n",
      "400 loss: 0.97926813\n",
      "401 loss: 0.9791117\n",
      "402 loss: 0.9789553\n",
      "403 loss: 0.97879905\n",
      "404 loss: 0.97864264\n",
      "405 loss: 0.9784863\n",
      "406 loss: 0.9783299\n",
      "407 loss: 0.9781737\n",
      "408 loss: 0.9780174\n",
      "409 loss: 0.9778613\n",
      "410 loss: 0.9777052\n",
      "411 loss: 0.97754896\n",
      "412 loss: 0.97739303\n",
      "413 loss: 0.9772368\n",
      "414 loss: 0.9770808\n",
      "415 loss: 0.9769249\n",
      "416 loss: 0.9767689\n",
      "417 loss: 0.97661304\n",
      "418 loss: 0.9764572\n",
      "419 loss: 0.9763014\n",
      "420 loss: 0.9761456\n",
      "421 loss: 0.9759898\n",
      "422 loss: 0.9758342\n",
      "423 loss: 0.9756784\n",
      "424 loss: 0.9755227\n",
      "425 loss: 0.975367\n",
      "426 loss: 0.97521144\n",
      "427 loss: 0.97505593\n",
      "428 loss: 0.97490036\n",
      "429 loss: 0.9747449\n",
      "430 loss: 0.97458947\n",
      "431 loss: 0.974434\n",
      "432 loss: 0.97427875\n",
      "433 loss: 0.97412336\n",
      "434 loss: 0.97396797\n",
      "435 loss: 0.97381264\n",
      "436 loss: 0.9736574\n",
      "437 loss: 0.9735022\n",
      "438 loss: 0.97334707\n",
      "439 loss: 0.97319186\n",
      "440 loss: 0.973037\n",
      "441 loss: 0.97288173\n",
      "442 loss: 0.9727267\n",
      "443 loss: 0.9725717\n",
      "444 loss: 0.97241676\n",
      "445 loss: 0.97226167\n",
      "446 loss: 0.97210675\n",
      "447 loss: 0.9719519\n",
      "448 loss: 0.97179705\n",
      "449 loss: 0.9716422\n",
      "450 loss: 0.9714874\n",
      "451 loss: 0.97133267\n",
      "452 loss: 0.971178\n",
      "453 loss: 0.97102326\n",
      "454 loss: 0.9708687\n",
      "455 loss: 0.97071403\n",
      "456 loss: 0.9705593\n",
      "457 loss: 0.9704048\n",
      "458 loss: 0.9702503\n",
      "459 loss: 0.9700957\n",
      "460 loss: 0.9699415\n",
      "461 loss: 0.96978694\n",
      "462 loss: 0.9696325\n",
      "463 loss: 0.9694782\n",
      "464 loss: 0.96932393\n",
      "465 loss: 0.96916944\n",
      "466 loss: 0.96901536\n",
      "467 loss: 0.96886104\n",
      "468 loss: 0.96870685\n",
      "469 loss: 0.96855253\n",
      "470 loss: 0.96839845\n",
      "471 loss: 0.96824455\n",
      "472 loss: 0.9680903\n",
      "473 loss: 0.96793616\n",
      "474 loss: 0.96778226\n",
      "475 loss: 0.96762824\n",
      "476 loss: 0.9674743\n",
      "477 loss: 0.9673204\n",
      "478 loss: 0.9671664\n",
      "479 loss: 0.9670126\n",
      "480 loss: 0.9668587\n",
      "481 loss: 0.9667051\n",
      "482 loss: 0.9665512\n",
      "483 loss: 0.96639735\n",
      "484 loss: 0.96624374\n",
      "485 loss: 0.96609\n",
      "486 loss: 0.9659365\n",
      "487 loss: 0.96578276\n",
      "488 loss: 0.9656293\n",
      "489 loss: 0.9654757\n",
      "490 loss: 0.96532214\n",
      "491 loss: 0.96516854\n",
      "492 loss: 0.9650151\n",
      "493 loss: 0.96486175\n",
      "494 loss: 0.9647084\n",
      "495 loss: 0.9645551\n",
      "496 loss: 0.9644016\n",
      "497 loss: 0.9642483\n",
      "498 loss: 0.964095\n",
      "499 loss: 0.9639419\n",
      "500 loss: 0.9637886\n",
      "501 loss: 0.96363544\n",
      "502 loss: 0.9634822\n",
      "503 loss: 0.96332914\n",
      "504 loss: 0.9631761\n",
      "505 loss: 0.96302307\n",
      "506 loss: 0.96287006\n",
      "507 loss: 0.96271694\n",
      "508 loss: 0.9625641\n",
      "509 loss: 0.962411\n",
      "510 loss: 0.9622581\n",
      "511 loss: 0.9621052\n",
      "512 loss: 0.9619524\n",
      "513 loss: 0.9617996\n",
      "514 loss: 0.96164685\n",
      "515 loss: 0.961494\n",
      "516 loss: 0.9613412\n",
      "517 loss: 0.96118855\n",
      "518 loss: 0.96103585\n",
      "519 loss: 0.96088326\n",
      "520 loss: 0.9607306\n",
      "521 loss: 0.9605781\n",
      "522 loss: 0.96042556\n",
      "523 loss: 0.960273\n",
      "524 loss: 0.9601204\n",
      "525 loss: 0.9599681\n",
      "526 loss: 0.95981556\n",
      "527 loss: 0.95966315\n",
      "528 loss: 0.9595108\n",
      "529 loss: 0.9593586\n",
      "530 loss: 0.95920616\n",
      "531 loss: 0.9590538\n",
      "532 loss: 0.95890164\n",
      "533 loss: 0.9587494\n",
      "534 loss: 0.95859724\n",
      "535 loss: 0.958445\n",
      "536 loss: 0.95829284\n",
      "537 loss: 0.9581406\n",
      "538 loss: 0.9579887\n",
      "539 loss: 0.9578367\n",
      "540 loss: 0.9576846\n",
      "541 loss: 0.9575326\n",
      "542 loss: 0.95738065\n",
      "543 loss: 0.9572287\n",
      "544 loss: 0.9570768\n",
      "545 loss: 0.95692486\n",
      "546 loss: 0.9567732\n",
      "547 loss: 0.9566211\n",
      "548 loss: 0.95646954\n",
      "549 loss: 0.9563176\n",
      "550 loss: 0.95616597\n",
      "551 loss: 0.9560141\n",
      "552 loss: 0.9558626\n",
      "553 loss: 0.955711\n",
      "554 loss: 0.9555594\n",
      "555 loss: 0.9554078\n",
      "556 loss: 0.95525616\n",
      "557 loss: 0.9551046\n",
      "558 loss: 0.95495325\n",
      "559 loss: 0.9548017\n",
      "560 loss: 0.95465016\n",
      "561 loss: 0.95449865\n",
      "562 loss: 0.9543474\n",
      "563 loss: 0.9541961\n",
      "564 loss: 0.9540446\n",
      "565 loss: 0.95389336\n",
      "566 loss: 0.9537421\n",
      "567 loss: 0.9535909\n",
      "568 loss: 0.95343965\n",
      "569 loss: 0.95328856\n",
      "570 loss: 0.9531372\n",
      "571 loss: 0.9529862\n",
      "572 loss: 0.95283514\n",
      "573 loss: 0.95268404\n",
      "574 loss: 0.95253295\n",
      "575 loss: 0.95238197\n",
      "576 loss: 0.9522311\n",
      "577 loss: 0.95208\n",
      "578 loss: 0.9519291\n",
      "579 loss: 0.9517782\n",
      "580 loss: 0.9516274\n",
      "581 loss: 0.95147645\n",
      "582 loss: 0.95132565\n",
      "583 loss: 0.9511749\n",
      "584 loss: 0.95102406\n",
      "585 loss: 0.9508733\n",
      "586 loss: 0.9507227\n",
      "587 loss: 0.95057195\n",
      "588 loss: 0.95042145\n",
      "589 loss: 0.9502707\n",
      "590 loss: 0.9501202\n",
      "591 loss: 0.9499695\n",
      "592 loss: 0.94981897\n",
      "593 loss: 0.9496685\n",
      "594 loss: 0.94951797\n",
      "595 loss: 0.9493674\n",
      "596 loss: 0.94921714\n",
      "597 loss: 0.9490667\n",
      "598 loss: 0.9489163\n",
      "599 loss: 0.94876605\n",
      "600 loss: 0.9486156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
